{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":71881,"status":"ok","timestamp":1658497567990,"user":{"displayName":"pavitra m pavitra","userId":"17187811436046255843"},"user_tz":-330},"id":"-EDYbTnijXKx","outputId":"d1f52342-9e79-4149-de55-5b2fc5429cbb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","\n"," \n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tensorflow.python.keras import models, layers, optimizers\n","import tensorflow\n","from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import bz2\n","from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n","import re\n","\n","%matplotlib inline\n","\n","# Input data files are available in the \"../input/\" directory.\n","# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Any results you write to the current directory are saved as output."]},{"cell_type":"markdown","metadata":{"id":"uFwQW_dtjXK1"},"source":["## Reading the text\n","\n","The text is held in a compressed format. Luckily, we can still read it line by line. The first word gives the label, so we have to convert that into a number and then take the rest to be the comment."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8dg_OXFFjXK3"},"outputs":[],"source":["def get_labels_and_texts(file):\n","    labels = []\n","    texts = []\n","    for line in bz2.BZ2File(file):\n","        x = line.decode(\"utf-8\")\n","        labels.append(int(x[9]) - 1)\n","        texts.append(x[10:].strip())\n","    return np.array(labels), texts\n","train_labels, train_texts = get_labels_and_texts('/content/drive/MyDrive/Amazon /train.ft.txt.bz2')\n","test_labels, test_texts = get_labels_and_texts('/content/drive/MyDrive/Amazon /test.ft.txt.bz2')"]},{"cell_type":"markdown","metadata":{"id":"6IfVV9pkjXK4"},"source":["## Text Preprocessing\n","\n","The first thing I'm going to do to process the text is to lowercase everything and then remove non-word characters. I replace these with spaces since most are going to be punctuation. Then I'm going to just remove any other characters (like letters with accents). It could be better to replace some of these with regular ascii characters but I'm just going to ignore that here. It also turns out if you look at the counts of the different characters that there are very few unusual characters in this corpus."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sZ_cJI7VjXK5"},"outputs":[],"source":["import re\n","NON_ALPHANUM = re.compile(r'[\\W]')\n","NON_ASCII = re.compile(r'[^a-z0-1\\s]')\n","def normalize_texts(texts):\n","    normalized_texts = []\n","    for text in texts:\n","        lower = text.lower()\n","        no_punctuation = NON_ALPHANUM.sub(r' ', lower)\n","        no_non_ascii = NON_ASCII.sub(r'', no_punctuation)\n","        normalized_texts.append(no_non_ascii)\n","    return normalized_texts\n","        \n","train_texts = normalize_texts(train_texts)\n","test_texts = normalize_texts(test_texts)"]},{"cell_type":"markdown","metadata":{"id":"_5oe2cgijXK8"},"source":["## Train/Validation Split\n","Now I'm going to set aside 20% of the training set for validation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C2_xGTpIjXK_"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","train_texts, val_texts, train_labels, val_labels = train_test_split(\n","    train_texts, train_labels, random_state=57643892, test_size=0.2)"]},{"cell_type":"markdown","metadata":{"id":"WvZuaGX4jXLC"},"source":["Keras provides some tools for converting text to formats that are useful in deep learning models. I've already done some processing, so now I will just run a Tokenizer using the top 12000 words as features."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"br-v2uQUjXLF"},"outputs":[],"source":["MAX_FEATURES = 12000\n","tokenizer = Tokenizer(num_words=MAX_FEATURES)\n","tokenizer.fit_on_texts(train_texts)\n","train_texts = tokenizer.texts_to_sequences(train_texts)\n","val_texts = tokenizer.texts_to_sequences(val_texts)\n","test_texts = tokenizer.texts_to_sequences(test_texts)\n"]},{"cell_type":"markdown","metadata":{"id":"iLFUz3DXjXLG"},"source":["## Padding Sequences\n","In order to use batches effectively, I'm going to need to take my sequences and turn them into sequences of the same length. I'm just going to make everything here the length of the longest sentence in the training set. I'm not dealing with this here, but it may be advantageous to have variable lengths so that each batch contains sentences of similar lengths. This might help mitigate issues that arise from having too many padded elements in a sequence. There are also different padding modes that might be useful for different models.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5pby9OKIjXLH"},"outputs":[],"source":["MAX_LENGTH = max(len(train_ex) for train_ex in train_texts)\n","train_texts = pad_sequences(train_texts, maxlen=MAX_LENGTH)\n","val_texts = pad_sequences(val_texts, maxlen=MAX_LENGTH)\n","test_texts = pad_sequences(test_texts, maxlen=MAX_LENGTH)\n"]},{"cell_type":"markdown","metadata":{"id":"AR7cGNsVjXLJ"},"source":["## Convolutional Neural Net Model\n","\n","I'm just using fairly simple models here. This CNN has an embedding with a dimension of 64, 3 convolutional layers with the first two having match normalization and max pooling and the last with global max pooling. The results are then passed to a dense layer and then the output."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1658498360024,"user":{"displayName":"pavitra m pavitra","userId":"17187811436046255843"},"user_tz":-330},"id":"fxPhJRftVtSa","outputId":"277c0e29-f285-456c-8e65-2e6e7fbf1609"},"outputs":[{"output_type":"stream","name":"stdout","text":["2.8.2\n"]}],"source":["import tensorflow as tf\n","print(tf.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":82019,"status":"ok","timestamp":1658498442029,"user":{"displayName":"pavitra m pavitra","userId":"17187811436046255843"},"user_tz":-330},"id":"4jB9yqDGV1he","outputId":"82b6d978-27a7-44e5-ae0b-d84c3835c0fc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.2+zzzcolab20220719082949)\n","Collecting tensorflow\n","  Downloading tensorflow-2.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n","\u001b[K     |████████████████████████████████| 511.7 MB 5.7 kB/s \n","\u001b[?25hCollecting gast<=0.4.0,>=0.2.1\n","  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.26.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.1)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n","Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n","  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n","\u001b[K     |████████████████████████████████| 438 kB 58.9 MB/s \n","\u001b[?25hCollecting keras<2.10.0,>=2.9.0rc0\n","  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n","\u001b[K     |████████████████████████████████| 1.6 MB 47.1 MB/s \n","\u001b[?25hRequirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.1)\n","Collecting tensorboard<2.10,>=2.9\n","  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n","\u001b[K     |████████████████████████████████| 5.8 MB 46.8 MB/s \n","\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.1.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow) (21.3)\n","Collecting flatbuffers<2,>=1.12\n","  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.47.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.35.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.23.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (4.12.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.8.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.10)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow) (3.0.9)\n","Installing collected packages: tensorflow-estimator, tensorboard, keras, gast, flatbuffers, tensorflow\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.8.0\n","    Uninstalling tensorflow-estimator-2.8.0:\n","      Successfully uninstalled tensorflow-estimator-2.8.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.8.0\n","    Uninstalling tensorboard-2.8.0:\n","      Successfully uninstalled tensorboard-2.8.0\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.8.0\n","    Uninstalling keras-2.8.0:\n","      Successfully uninstalled keras-2.8.0\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.5.3\n","    Uninstalling gast-0.5.3:\n","      Successfully uninstalled gast-0.5.3\n","  Attempting uninstall: flatbuffers\n","    Found existing installation: flatbuffers 2.0\n","    Uninstalling flatbuffers-2.0:\n","      Successfully uninstalled flatbuffers-2.0\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.8.2+zzzcolab20220719082949\n","    Uninstalling tensorflow-2.8.2+zzzcolab20220719082949:\n","      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220719082949\n","Successfully installed flatbuffers-1.12 gast-0.4.0 keras-2.9.0 tensorboard-2.9.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["flatbuffers","gast","keras","tensorboard","tensorflow"]}}},"metadata":{}}],"source":["pip install --upgrade tensorflow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HjJdk8mGjXLK"},"outputs":[],"source":["#tf.keras.layers.experimental.preprocessing.Normalization\n","#from tensorflow.keras.layers import BatchNormalization\n","def build_model():\n","    sequences = layers.Input(shape=(MAX_LENGTH,))\n","    embedded = layers.Embedding(MAX_FEATURES, 64)(sequences)\n","    x = layers.Conv1D(64, 3, activation='relu')(embedded)\n","   # x = layers.BatchNormalization()(x)\n","    x = layers.MaxPool1D(3)(x)\n","    x = layers.Conv1D(64, 5, activation='relu')(x)\n","   # x = layers.BatchNormalization()(x)\n","    x = layers.MaxPool1D(5)(x)\n","    x = layers.Conv1D(64, 5, activation='relu')(x)\n","    x = layers.GlobalMaxPool1D()(x)\n","    x = layers.Flatten()(x)\n","    x = layers.Dense(100, activation='relu')(x)\n","    predictions = layers.Dense(1, activation='sigmoid')(x)\n","    model = models.Model(inputs=sequences, outputs=predictions)\n","    model.compile(\n","        optimizer='rmsprop',\n","        loss='binary_crossentropy',\n","        metrics=['binary_accuracy']\n","    )\n","    return model\n","    \n","model = build_model()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LN6ks5YtjXLL","executionInfo":{"status":"ok","timestamp":1658505169525,"user_tz":-330,"elapsed":6726866,"user":{"displayName":"pavitra m pavitra","userId":"17187811436046255843"}},"outputId":"2779f064-c6d6-44bf-c71b-ce5d47e98e9a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","22500/22500 [==============================] - 3389s 151ms/step - loss: 0.1718 - binary_accuracy: 0.9348 - val_loss: 0.1567 - val_binary_accuracy: 0.9422\n","Epoch 2/2\n","22500/22500 [==============================] - 3337s 148ms/step - loss: 0.1564 - binary_accuracy: 0.9430 - val_loss: 0.1583 - val_binary_accuracy: 0.9431\n"]},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f28a6ea34d0>"]},"metadata":{},"execution_count":10}],"source":["model.fit(\n","    train_texts, \n","    train_labels, \n","    batch_size=128,\n","    epochs=2,\n","    validation_data=(val_texts, val_labels), )"]},{"cell_type":"markdown","metadata":{"id":"zDVfBnmJjXLM"},"source":["Once this finishes training, we should find that we get an accuracy of around 94% for this model."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8teWVk3IjXLN","executionInfo":{"status":"ok","timestamp":1658505371880,"user_tz":-330,"elapsed":202397,"user":{"displayName":"pavitra m pavitra","userId":"17187811436046255843"}},"outputId":"8434c380-b474-4ee7-ebb2-29118449d1e4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy score: 0.9432\n","F1 score: 0.9421\n","ROC AUC score: 0.9855\n"]}],"source":["preds = model.predict(test_texts)\n","print('Accuracy score: {:0.4}'.format(accuracy_score(test_labels, 1 * (preds > 0.5))))\n","print('F1 score: {:0.4}'.format(f1_score(test_labels, 1 * (preds > 0.5))))\n","print('ROC AUC score: {:0.4}'.format(roc_auc_score(test_labels, preds)))"]},{"cell_type":"markdown","metadata":{"id":"I02mKnhdjXLP"},"source":["## Recurrent Neural Net Model\n","For an RNN model I'm also going to use a simple model. This has an embedding, two GRU layers, followed by 2 dense layers and then the output layer. I'm using the CuDNNGRU rather than GRU because the former will run much faster (over a factor of 10 I think on Kaggle's servers."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gnNt03hxM-T9"},"outputs":[],"source":["def build_rnn_model():\n","    sequences = layers.Input(shape=(MAX_LENGTH,))\n","    embedded = layers.Embedding(MAX_FEATURES, 64)(sequences)\n","    x = layers.CuDNNGRU(128, return_sequences=True)(embedded)\n","    x = layers.CuDNNGRU(128)(x)\n","    x = layers.Dense(32, activation='relu')(x)\n","    x = layers.Dense(100, activation='relu')(x)\n","    predictions = layers.Dense(1, activation='sigmoid')(x)\n","    model = models.Model(inputs=sequences, outputs=predictions)\n","    model.compile(\n","        optimizer='rmsprop',\n","        loss='binary_crossentropy',\n","        metrics=['binary_accuracy']\n","    )\n","    return model\n","    \n","    rnn_model = build_rnn_model()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GhXMfcY2jXLR"},"outputs":[],"source":["def build_rnn_model():\n","    rnn_model.fit(\n","    train_texts, \n","    train_labels, \n","    batch_size=128,\n","    epochs=1,\n","    validation_data=(val_texts, val_labels), )"]},{"cell_type":"markdown","metadata":{"id":"qRLfACZfjXLS"},"source":["And we should find that this model will end up with an accuracy similar to the CNN model. I haven't bothered to set the seeds, but it can go as high as 95%."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SWT3IiaojXLS","executionInfo":{"status":"ok","timestamp":1658505486798,"user_tz":-330,"elapsed":609,"user":{"displayName":"pavitra m pavitra","userId":"17187811436046255843"}},"outputId":"f79decdd-b5b1-469c-a4de-dd3d106b405f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy score: 0.9432\n","F1 score: 0.9421\n","ROC AUC score: 0.9855\n"]}],"source":["def rnn_model():\n"," preds = rnn_model.predict(test_texts)\n","print('Accuracy score: {:0.4}'.format(accuracy_score(test_labels, 1 * (preds > 0.5))))\n","print('F1 score: {:0.4}'.format(f1_score(test_labels, 1 * (preds > 0.5))))\n","print('ROC AUC score: {:0.4}'.format(roc_auc_score(test_labels, preds)))"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"sentiment-analysis-CNN-RNN-amazon-reviews.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":0}